{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8dhhae7t8sD0G3Q+sPOpv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LalithShankar/BERT-for-Sentiment-Analysis/blob/master/Topic%20extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMp_rkagHWO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shorttext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjo0EbyIHaAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stemming.porter import stem\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1hV7Tp3HcR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = [lambda s: re.sub('[^\\w\\s]', '', s),\n",
        "            lambda s: re.sub('[\\d]', '', s),\n",
        "            lambda s: s.lower(),\n",
        "            \n",
        " ]\n",
        "txtpreproceesor = shorttext.utils.text_preprocessor(pipeline)\n",
        "txtpreproceesor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci0Zfd9GHgXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docx = pd.read_csv('Rule_Based_Sentiment.csv')\n",
        "docx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J4BORfdHjEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docids = list(usprezdf['index'])\n",
        "corpus = [txtpreproceesor(Data).split(' ') for Data in usprezdf['Data']]\n",
        "# corpus = [txtpreproceesor(Data).split(' ') for Data in usprezdf['Data']]\n",
        "corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cXqtcAHj9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtm = shorttext.utils.DocumentTermMatrix(corpus, docids=docids, tfidf=False)\n",
        "print(dtm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS9MLxvhHnWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtm.get_doc_tokens('paul')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7H7eXJdHoFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtm.get_doc_frequency(stem('change'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkI93WoGHsPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install gensimpip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVnQQ6unHtom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import matutils, models\n",
        "import scipy.sparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFIj3aNAHvnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tdm = usprezdf.transpose()\n",
        "tdm.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDdMPWwHHxl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "def nouns(text):\n",
        "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
        "    is_noun = lambda pos: pos[:2] == 'NN'\n",
        "    tokenized = word_tokenize(text)\n",
        "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
        "    return ' '.join(all_nouns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQrAslwnH1x1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS4b6OlfH33G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data_nouns = pd.DataFrame(docx.Data.apply(nouns))\n",
        "data_nouns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymMW5FkJH42D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new document-term matrix using only nouns\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Re-add the additional stop words since we are recreating the document-term matrix\n",
        "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
        "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "\n",
        "# Recreate a document-term matrix with only nouns\n",
        "cvn = CountVectorizer(stop_words=stop_words)\n",
        "data_cvn = cvn.fit_transform(data_nouns.Data)\n",
        "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
        "data_dtmn.index = data_nouns.index\n",
        "data_dtmn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4woQXEdJH66V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the gensim corpus\n",
        "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
        "\n",
        "# Create the vocabulary dictionary\n",
        "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())\n",
        "id2wordn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DStMYHuMH8kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's start with 2 topics\n",
        "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
        "ldan.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqtlif8jH_5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try topics = 3\n",
        "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
        "ldan.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlV5LQv-IB8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try 4 topics\n",
        "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
        "ldan.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_FPoLg8ICc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create a function to pull out nouns from a string of text\n",
        "def nouns_adj(text):\n",
        "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
        "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
        "    tokenized = word_tokenize(text)\n",
        "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
        "    return ' '.join(nouns_adj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMMPFPrDIGY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the nouns function to the transcripts to filter only on nouns\n",
        "data_nouns_adj = pd.DataFrame(docx.Data.apply(nouns_adj))\n",
        "data_nouns_adj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg7Ji6C2IG-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
        "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
        "data_cvna = cvna.fit_transform(data_nouns_adj.Data)\n",
        "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
        "data_dtmna.index = data_nouns_adj.index\n",
        "data_dtmna"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAWWkT3xII0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the gensim corpus\n",
        "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
        "\n",
        "# Create the vocabulary dictionary\n",
        "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpDxBlNNIK7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's start with 2 topics\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
        "ldana.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO7ogAH4IOOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try 3 topics\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
        "ldana.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxqdYCyZIO3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try 4 topics\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
        "ldana.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OFeyZ0yIQnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our final LDA model (for now)\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
        "ldana.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXhtgJcTITrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Let's take a look at which topics each transcript contains\n",
        "corpus_transformed = ldana[corpusna]\n",
        "d = list(zip([a for [(a, b)] in corpus_transformed], data_dtmna.index))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}